{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of BiLSTM_Doc_Smelling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"MpxvQ14HFYM1","executionInfo":{"status":"ok","timestamp":1603372730922,"user_tz":-360,"elapsed":23776,"user":{"displayName":"Md. Tawkat Islam Khondaker","photoUrl":"","userId":"12040636431562290492"}},"outputId":"11305bfe-0d0b-45ed-b975-f915a9103f12","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jGuiOxBdFh3k"},"source":["!pip install -q keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utYxkMR9F0wq"},"source":["!pip install -q pydrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIJwvszjF4AZ"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRjYe1E_DNdu"},"source":["project_path='/content/drive/My Drive/Documentation Smelling/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"18swS5-OzmZA"},"source":["import re\n","import string\n","import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk import re, SnowballStemmer\n","\n","def clean_text(text):\n","    import nltk\n","    nltk.download('stopwords')\n","    ## Remove puncuation\n","    #text = text.translate(string.punctuation)\n","    translate_table = dict((ord(char), None) for char in string.punctuation)\n","    text = text.translate(translate_table)\n","\n","\n","\n","    text = re.sub(r\"\\n\", \" \", text)\n","    text = re.sub(r\"\\r\", \" \", text)\n","\n","    ## Convert words to lower case and split them\n","    text = text.lower().split()\n","\n","    ## Remove stop words\n","    stops = set(stopwords.words(\"english\"))\n","    text = [w for w in text if not w in stops and len(w) >= 3]\n","\n","    text = \" \".join(text)\n","    ## Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    ## Stemming\n","    text = text.split()\n","    stemmer = SnowballStemmer('english')\n","    stemmed_words = [stemmer.stem(word) for word in text]\n","    text = \" \".join(stemmed_words)\n","\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AD8erKnQz53P"},"source":["from keras.models import Sequential,load_model\n","from keras.layers import Dense,Dropout\n","from keras.layers import LSTM,Conv1D, Bidirectional\n","from keras.layers import MaxPooling1D\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from keras import optimizers\n","from keras.layers import TimeDistributed\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pickle\n","\n","from sklearn.preprocessing import LabelEncoder\n","from keras import callbacks\n","\n","def create_model_BiLSTM(vocabulary_size,embedding_size,embedding_matrix,num_class=5):\n","    ## create model\n","    model = Sequential()\n","    model.add(Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix],trainable=False))\n","\n","    model.add(Bidirectional(LSTM(300)))\n","\n","\n","    model.add(Dense(num_class, activation='sigmoid'))\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOkR7D991s_c"},"source":["import pandas as pd\n","\n","df = pd.read_excel(project_path + 'Datasets/dataset.xlsx')\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYGBkoLT0bAP"},"source":["import pickle\n","\n","\n","texts=df['Documentation Text']\n","texts=texts.map(lambda x: clean_text(x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhLPBSV92jQw"},"source":["label=df.iloc[:,1:6].values\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaO6hCLh2k6d"},"source":["with open(project_path + 'Datasets/pickle_clean_text_Xy.pickle','wb') as f:\n","    pickle._dump((texts,label),f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIJ02Rjg3-q5"},"source":["from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","import pandas as pd\n","import numpy as np\n","import pickle\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","vocabulary_size = 400000\n","#***********\n","time_step=300\n","embedding_size=100\n","\n","pickle_train=open(project_path + 'Datasets/pickle_clean_text_Xy.pickle','rb')\n","texts,y_train=pickle.load(pickle_train)\n","\n","tokenizer_train=Tokenizer(num_words=vocabulary_size)\n","tokenizer_train.fit_on_texts(texts)\n","encoded_train=tokenizer_train.texts_to_sequences(texts=texts)\n","#print(encoded_docs)\n","vocab_size_train = len(tokenizer_train.word_index) + 1\n","print(vocab_size_train)\n","\n","X_train = sequence.pad_sequences(encoded_train, maxlen=time_step,padding='post')\n","\n","\n","f = open(project_path + 'glove.6B.100d.txt',encoding='utf-8')\n","embeddings_train={}\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_train[word] = coefs\n","f.close()\n","\n","print('Total %s word vectors.' % len(embeddings_train))\n","\n","# create a weight matrix for words in training docs\n","embedding_matrix = np.zeros((vocab_size_train, embedding_size))\n","for word, i in tokenizer_train.word_index.items():\n","\tembedding_vector_train = embeddings_train.get(word)\n","\tif embedding_vector_train is not None:\n","\t\tembedding_matrix[i] = embedding_vector_train\n","\n","\n","with open(project_path + 'Datasets/pickle_doc_embedding_100dim.pickle','wb') as f:\n","    pickle.dump((X_train,y_train,embedding_matrix),f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdCSH1b1e86c"},"source":["!pip install -q iterative-stratification\n","\n","import glob\n","\n","import os\n","#from sklearn.model_selection import KFold\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","import gc\n","import keras.backend as K\n","import numpy as np\n","\n","pickle_load=open(project_path + 'Datasets/pickle_doc_embedding_100dim.pickle','rb')\n","X_train_all,y_train_all,embedding_matrix=pickle.load(pickle_load)\n","\n","print(X_train_all.shape)\n","print(y_train_all.shape)\n","\n","num_cross_validation = 5\n","\n","\n","mskf = MultilabelStratifiedKFold(n_splits = num_cross_validation, shuffle=True, random_state=42)\n","pred_list=[]\n","cvscores = []\n","y_true_all_fold = []\n","pred_binary_all_fold = []\n","\n","Fold = 1\n","\n","for train, val in mskf.split(X_train_all, y_train_all):\n","    gc.collect()\n","    K.clear_session()\n","    print('Fold: ', Fold)\n","\n","    X_train = X_train_all[train]\n","    X_val = X_train_all[val]\n","    y_train = y_train_all[train]\n","    y_val = y_train_all[val]\n","\n","    vocabulary_size=embedding_matrix.shape[0]\n","    model=create_model_BiLSTM(vocabulary_size,embedding_size,embedding_matrix, num_class=5)\n","    history=model.fit(X_train,y_train,batch_size=256,epochs=10,validation_split=0.1,shuffle=True)\n","\n","\n","\n","    pred=model.predict(X_val)\n","\n","\n","    pred_binary=np.array(pred)\n","    for i in range(len(pred_binary)):\n","      for j in range(len(pred_binary[i])):\n","        pred_binary[i][j]=int(1*(pred_binary[i][j]>0.5))\n","\n","      y_true_all_fold.append(y_val[i])\n","      pred_binary_all_fold.append(pred_binary[i])\n","\n","    Fold = Fold + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hg57-9cfPEn"},"source":["with open(project_path + 'Predictions/CV_2/pickle_Pred_doc_Bi_LSTM.pickle','wb') as f:\n","    pickle.dump((y_true_all_fold,pred_binary_all_fold),f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cc_mwGskfcNY"},"source":["import pickle\n","with open(project_path + 'Predictions/CV_2/pickle_Pred_doc_Bi_LSTM.pickle', 'rb') as f:\n","  y_test, pred_binary = pickle.load(f)\n","\n","print(len(y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HLfsfcsfi_n"},"source":["from sklearn.metrics import classification_report,precision_recall_fscore_support\n","from sklearn.metrics import precision_score,recall_score,f1_score\n","from sklearn.metrics import accuracy_score,jaccard_similarity_score, hamming_loss\n","\n","report=classification_report(y_test,pred_binary)\n","#report=precision_recall_fscore_support(y_test,pred_binary,average='micro')\n","print('Classification Report: '+str(report))\n","\n","\n","precision=precision_score(y_test,pred_binary,average='weighted')\n","print('Weighted Precision: '+str(precision))\n","\n","recall=recall_score(y_test,pred_binary,average='weighted')\n","print('Weighted Recall: '+str(recall))\n","\n","f1=f1_score(y_test,pred_binary,average='weighted')\n","print('Weighted F1 Score: '+str(f1))\n","\n","\n","\n","acc_hard=accuracy_score(y_test,pred_binary)\n","print('Hard Accuracy: '+str(acc_hard))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yM10gYjBfq8P"},"source":["y_true_np = np.array(y_test)\n","y_pred_np = np.array(pred_binary)\n","\n","num_classes = 5\n","for i in range(num_classes):\n","  print('Class: ' + str(i))\n","  new_y_true = y_true_np[:,i]\n","  new_y_pred = y_pred_np[:,i]\n","\n","  print('Accuracy for class ' + str(i) + ': ' + str(accuracy_score(new_y_true,new_y_pred)))\n","  print('Classification Report for class' + str(i) + ': ' + str(classification_report(new_y_true,new_y_pred)))"],"execution_count":null,"outputs":[]}]}